\section{Image Segmentation Architectures}

    Fully convolutional network (FCN) is a CNN variant which is a turning point for semantic segmentation literature \cite{long2015fully}.
    After that, many variants of CNN for segmentation have been developed.
    In this section, commonly used CNNs which are used for image segmentation starting from FCN are examined.

    \subsection{Fully Convolutional Network}

        Fully convolutional networks (FCNs) indicate that the convolutional neural networks are obtained by dismantling the fully connected layers from deep CNNs \cite{ulku2019survey}.
        FCNs are built on traditional classification networks such as VGG \cite{simonyan2014very}, AlexNet \cite{krizhevsky2012imagenet}, GoogLeNet \cite{szegedy2014going}, and ResNet \cite{he2016deep}.

        Convolutional layers are used instead of fully connected layers to produce outputs with the same size of inputs  instead of classification scores which are the outputs of CNNs.
        FCNs consist of two units encoding and decoding. Convolution and subsampling operations are performed in the encoding unit to encode the lower dimensional latent space.
        Deconvolution and upsampling are performed in the decoding unit which guarantee the same size of output with the input.
        Since FCNs do not include fully connected layers, it is faster to get an image with respect to the classical CNNs.

        Besides the including convolutional layers, skip architecture is one of the main reasons that makes FCNs faster over CNNs.
        Skip architectures help to prevent losing some information which can be lost becauseSkip connections are also p of the dropout or any other architectural decisions which may cause losing information.
        They provide flowing the summed or concatenated data between downsampling and upsamling blocks.
        Skip connections preserve the localised information which might be lost in pooling layers with bypassing them.

    \subsection{SegNet}

        \citet{badrinarayanan2017segnet} proposed a FCN based network architecture, called SegNet, aiming to increase the accuracy of segmentation tasks.
        As it can be seen in Figure~\ref{figure:segnet-architecture}, the encoder network consists of 13 convolutional layers of VGG16 network
        instead of the original fully connected layers of FCN. A pixel-wise classification layer is added to help the upsampling on the lower resolution images in the decoder network.
        The upsampling part is the novel improvement of SegNet. Encoder is not fully connected in Segnet. Then, it causes the train parameters to decrease by 90\%.
        There is a corresponding decoder for every 13 encoders and they are responsible for the upsampling of the feature map.

        \input{02-related-works/figures/segnet-architecture}

    \subsection{U-Net}\label{section:unet}

        \citet{ronneberger2015u} proposed a new CNN namely U-Net designed for medical imaging.
        Because medical image segmentation suffers from lack of large dataset, it is relatively hard to capture image context with localized lesions.
        U-Net aims to achieve competitive results even if the training data are relatively small.

        \input{02-related-works/figures/u-net-architecture}

        Classical feed-forward CNNs can learn many small information via the fully connected layers.
        Large datasets provide a number of parameters to train.
        However, they are often hard to gather or not accesible in medical domain.
        U-Net architecture provides more accurate results with smaller datasets by capturing
        the detailed context. As it can be shown in Figure~\ref{figure:unet-architecture}, U-Nets are made up of a compact hierarchy using upsampled and downsapled layers till the output layer.
        These layers are called as contracting and expanding layers respectively in Figure~\ref{figure:unet-architecture}.

        The purpose of the contracting path is to increase resolutions and learn features  to capture context while the role of the expanding path is to aid in precise localization with a series of upsampling operations.
        The contracting path consist of two three-by-three convolutions followed by a ReLU and two-by-two max pooling layers.
        On the other side, up convolution layers exist to upsample the outputs.
        Skip connections help to prevent to lose the spatial context combining with upsampled outputs by transfering the low resolution features to expanding path.
        The authors used a large-weighted loss function to separate boundaries of background labels and touching segments which is a known problem of medical image segmentation.

    \subsection{Generative Adversarial Network}

        \citet{goodfellow2014generative} proposed a deep learning framework which is called generative adversarial network (GAN) consisting of two neural networks namely generator and discriminator.
        The proposed network can be considered as an autoencoder trying to produce a fake version of the real data.

        The generator which is the first part of the GANs, generates a sample and the discriminator interprets the sample as a real or fake.
        The ‘real’ means that whether the source of the data is training set. The flow can be seen in Figure~\ref{figure:gan-architecture}.

        \input{02-related-works/figures/gan-architecture}

        It looks like a game where the generator tries to fool the discriminator with the samples it creates.
        The generator are update itself using the output of the discriminator on each iteration to get accurate results.
        GANs have proved its success in many image analysis tasks, such as creating very realistic synthetic images \cite{shrivastava2017learning},
        domain adaption \cite{bousmalis2017unsupervised} and data completion \cite{yeh2017semantic}.
